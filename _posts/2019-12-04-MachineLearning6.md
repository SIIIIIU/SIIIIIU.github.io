---
layout: article
title: 正则化
tags: Machine-Learning
article_header:
  type: overlay
typora-root-url: ..

---



<!--more-->

# 正则化

[TOC]

## 六、正则化

### 1、过拟合问题

本小节讲的是过拟合问题，主要是由于特征值过多，数据量过小导致无法约束特征值。

表现是模型在泛化一些新例子时表现极差，即无法预测新样本的值

![](/pic/MachineLearning/正则化之过拟合问题.png)

先以线性回归模型为例

先是最左边的模型，我们以一个一次函数来拟合这个数据集，画出的函数图像如图所示，表现为一个直线上升的直线。但根据训练集我们可以知道，随着房屋面积的不断扩大，价格的增加逐渐放缓，所以这个直线并没有很好的拟合好这个数据集，即欠拟合和高偏差。

而中间这个模型用了一个二次函数来拟合这个数据集，拟合的就非常好，

最右边的模型则是以一个四次函数来拟合这个数据集，一方面，它似乎很好的拟合了这个曲线，因为它通过了所有的数据集；但这是一条扭曲的曲线，它不停地上下波动，事实上我们并不认为这个一个预测房价的好模型，这个问题就被称为过度拟合，也称这个算法具有高方差。

==概括的说过度拟合将会在变量过多的时候出现，这时你的假设函数能很好地拟合数据，代价函数接近0甚至等于0，但你可能就会得到这样的曲线，它太过拟合训练集以至于无法泛化到新的样本中，无法预测新样本的价格（这里的泛化指的是一个假设模型应用到新样本的能力）。== 



![](/pic/MachineLearning/正则化之过拟合问题2.png)

同样地，在logistic回归中：

左边为欠拟合，中间则是拟合地很好的一个例子，右边则是过拟合，模型包含了所有的正类，并排除了所有的负类，但曲线波动剧烈，没有办法很好地泛化。



![](/pic/MachineLearning/正则化之过拟合问题3.png)

那么当过拟合发生时，我们可以怎样解决呢？

在前面的例子中，当我们使用一维或者二维数据时，我们可以通过绘出假设模型的图像来研究问题，再选择合适的多项式阶数。

但事实上更多的时候，我们的学习问题需要有很多的变量。当数据的维数如此之多的时候，不要说选择合适的阶数，光绘图可视化的难度便会非常大。

![](/pic/MachineLearning/正则化之过拟合问题4.png)

对于过拟合问题，实际上有两种方法来解决：

第一种便是减少特征值的数量。

减少特征值的数量我们可以通过手动选择哪些特征需要保留。或者使用模型选择算法（将在之后的课程中提到），这种算法可以自动选择哪些特征可以保留，哪些应该舍弃。

这种减少特征值数量的方法可以有效的减小过拟合问题的发生。但我们要意识到一点，如果减少特征值，我们同样也减少了获取到的数据集的信息。如果我们所有的特征值都是需要的，那么这种时候减少特征值尽管会减少过拟合问题的发生，但因为减少了获取到的数据集的信息，我们也很难拟合出一个很好的模型。

第二种选择便是正则化。正则化会保留所有的特征值，但会减少量级或者参数 $\theta_j$ 的值。这个方法很有效，当我们有很多特征值的时候，且每个特征值都能对预测的 $y$ 值产生影响时。



### 2、代价函数

![](/pic/MachineLearning/正则化之代价函数.png)

思考之前的例子，假设我们上帝视角，知道一个拟合得比较好的模型就是左图那样的二次模型，而右图这个四次函数之所过拟合是由于 $\theta_3x^3+\theta_4x^4$ 的影响，其实也就是特征值 $\theta_3,\theta_4$ ，那么就可以“惩罚”  $\theta_3,\theta_4$ ，即将其值减到最小。

具体做法就是可以将代价函数改成如下的样子：
$$
J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2+1000\theta_3^2+1000\theta_4^2
$$
这样就可以让 $\theta_3,\theta_4$ 的值减到最小，甚至可能接近0，那么其对模型的影响就非常小了，最后模型拟合出来的函数可能如右图洋红色线条所示，虽不是一条二次函数图像，但非常接近。

![](/pic/MachineLearning/正则化之代价函数1.png)

但之前我们说了我们是上帝视角，事实上在实际的学习问题中，我们并不知道哪些特征是应该对模型影响小的从而"惩罚"它们。那我们应该如何做呢？

事实上，正则化算法是将所有的参数 $\theta$ 的值减小。将所有的参值减小，模型的形状就会更光滑。

所以对于一个线性回归模型进行正则化，其代价函数应该是：
$$
J(\theta) = \frac{1}{2m}[\sum\limits_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})^2+\lambda\sum_{i=1}^m\theta_j^2]
$$
其中 $\lambda\sum_{i=1}^m\theta_j^2$ 我们是按照习惯来的所以从没有加 $\theta_0$ ，其实加不加对于模型的影响极小，所以不必过于纠结。

![](/pic/MachineLearning/正则化之代价函数2.png)

代价函数右边的 $\lambda\sum_{i=1}^m\theta_j^2$ 被称为正则项， $\lambda$ 即为正则参数， $\lambda$ 的作用就是控制两个目标之间的取舍。

第一个目标，和函数的第一项有关，即更好的拟合数据集；而第二个目标则与函数的第二项（正则项）有关，即要保持参数尽可能的小。==而正则化参数 $\lambda$ 的作用就是调节这两个目标之间的平衡，即 更好的去拟合训练集的目标和将参数控制得更小的目标，从而保持假设模型的相对简单，避免出现过拟合的情况。== 

![](/pic/MachineLearning/正则化之代价函数3.png)

如果我们将正则化参数 $\lambda$ 的值设置得过大时，那么参数 $\theta$ 的惩罚力度就会过大， $\theta$ 的值甚至都可能接近0，那么上述的模型就有可能退化成一条直线，这就是一个欠拟合的例子。==所以如果 $\lambda$ 的值过大，就有可能导致欠拟合，而 $\lambda$ 的值过小，就有可能过拟合。== 所以要选取一个合适的 $\lambda$ 。



### 3、线性回归的正则化

之前在线性回归模型中提到了两种算法，梯度下降和正规方程。

这节课我们就用这两种方法解决含有正则项的线性回归模型，即正则化线性回归模型。

![](/pic/MachineLearning/正则化之线性回归的正则化.png)

上图中是我们上节中提到的正则化的线性回归模型的代价函数。

其中方括号前面是一般线性回归模型中的代价函数，后面则是正则项，其中 $\lambda$ 为正则化参数。

而我们的问题就是求出合适的参数 $\theta$ ，使得代价函数 $J(\theta)$ 的值最小，即最小化代价函数 $J(\theta)$ 。

![](/pic/MachineLearning/正则化之线性回归的正则化2.png)

首先写出梯度下降的具体过程。

之前说的梯度下降中迭代的是  $\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)  \quad(j=0 ,1,2，... ，n)$ 。

而应用到正则化线性回归模型时，则是将正则化线性回归模型的代价函数代换到之前的代价函数 $J(\theta)$ 中，并注意将 $\theta_0$ 的情况作为特例单独提出来写，因为我们之前习惯上是惩罚 $\theta_1$ 到 $\theta_n$ 。

结果便为上图中 Repeat 中的括号中所写的那样，其中紫色框和蓝色框都是求偏导之后的结果。

然后将 $\theta_j$ 的式子中含有 $\theta_j$ 的项提出来整理一下，式子便如上图中最后一个式子。

其中 $\theta_j(1-\alpha \frac{\lambda}{m})$ 是个很有意思的项，因为一般来说 $\alpha$ 是个很小的数， $m$ 是个很大的数，所以 $(1-\alpha \frac{\lambda}{m})$ 是个非常接近1的数。而后面的式子其实和一般的线性回归模型一样。所以直观来说，正则化线性回归模型梯度下降算法的每次迭代就是将 $\theta_j$ 变小一点然后像一般的一样进行更新。

![](/pic/MachineLearning/正则化之线性回归的正则化3.png)

而对于正规方程方法中，之前的推导是将 $\frac{\partial}{\partial\theta_j}J(\theta) = 0$ 推导出来的，同理将正则化线性回归模型的代价函数带入便可推导出上述的式子，可直接求出最小话的参数 $\theta$ 。

![](/pic/MachineLearning/正则化之线性回归的正则化4.png)

而像之前一样考虑下正规方程不可逆的问题，事实上，只要正则化参数 $\lambda$ 的值大于0，那么括号中的式子就一定是可逆的，所以正则化线性回归模型也可以解决正规方程中不可逆的问题。



### 4、Logistic回归的正则化



![](/pic/MachineLearning/正则化之Logistic回归的正则化.png)

在之前的Logistic回归中，我们推出了上图所示的代价函数，而正则化的Logistic回归模型则是在之前的代价函数中加入这一项： $\frac{\lambda}{2m}\sum\limits_{j-1}^n\theta^2_j$ 。之后梯度下降也是将其代入就可以了。

![](/pic/MachineLearning/正则化之Logistic回归的正则化2.png)



![](/pic/MachineLearning/正则化之Logistic回归的正则化3.png)

而至于正则化Logistic回归的高级优化方法，可以参考之前Logistic回归中的高级优化方法，其实所用的函数，参数等都是一样的，只不过在定义代价函数时要写正则化Logistic回归的代价函数，以及之后求偏导要对应写一下。