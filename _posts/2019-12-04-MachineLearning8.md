---
layout: article
title: 神经网络参数的反向传播算法
tags: Machine-Learning
article_header:
  type: overlay
typora-root-url: ..

---



<!--more-->

# 神经网络参数的反向传播算法

[TOC]

## 八、神经网络参数的反向传播算法

### 1、代价函数

在本节和之后的几节，我们将讨论一个学习算法，它能在给定训练数据集时为神经网络拟合参数。

首先我们将从拟合神经网络的代价函数开始讲起。

![](/pic/MachineLearning/神经网络反向传播之代价函数.png)

我们将重点关注神经网络在分类问题的应用上。

首先我们定义几个概念：

我们定义训练集为 $(x^{(1)},y^{(1)})，(x^{(2)},y^{(2)})，...，(x^{(m)},y^{(m)})$ ，其中 $x^{(i)},y^{(i)}$ 都为向量。

定义 $L$ 为神经网络的总层数，如左上角的神经网络的层数 $L = 4$ 。

定义 $s_l$ 为第 $l$ 层的单元数（不包括偏置单元）。

分类问题分为两类，一类是二元分类问题，即输出结果 $y$ 只有 0 和 1 两个值，那么只需要一个输出单元。

另一类则是多元分类问题，假设有 $k$ 类（ $k\ge 3$ ），那么就有 $k$ 个输出单元，输出结果 $y$ 是一个 $k$ 维的向量。

![ ](/pic/MachineLearning/神经网络反向传播之代价函数2.png)

然后是代价函数，正则化逻辑回归的代价函数如上图所示：
$$
J(\theta) = -\frac{1}{m}[\sum\limits_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum\limits_{j=1}^n\theta^2_j
$$
其中后面的 $\frac{\lambda}{2m}\sum\limits_{j=1}^n\theta^2_j$ 是正则项。

然后我们推广到神经网络算法中，所谓代价函数，其实就可以理解为误差之和，而最小化代价函数，其实就是使误差之和最小，以达到更精确的概念。而之前只有一个输出结果，代价函数自然是那一个输出结果的误差之和，现在对于一个有 $k$ 类的多元分类问题，那么这个神经网络模型就会有 $k$ 个输出单元，也就有 $k$ 个输出结果，而代价函数又是误差之和，所以自然需要将每一个输出单元的误差相加，这便是神经网络的代价函数的前半部分。

至于后面的正则项，正则项是将所有的参数 $\theta$ 相加，而神经网络的参数矩阵 $\Theta$ 是一个三维矩阵=，因为每一层都有一个二维的参数矩阵，所以会有三个求和符号，这个地方注意下。

### 2、反向传播算法

![](/pic/MachineLearning/神经网络反向传播之反向传播.png)

那么具体如何使用神经网络算法呢？

根据我们之前的梯度下降或者其他的高级优化算法，我们其实只需要写出求出 $J(\Theta)$ 和其偏导项 $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$ 的代码就可以了。

使用上图中最上面的公式我们就可以求出 $J(\Theta)$ ，所以我们本节课的重点就是如何求这个偏导项。

![](/pic/MachineLearning/神经网络反向传播之反向传播2.png)

首先从最简单的例子说起，假设我们的数据集就是一个实数对，那么我们首先应该计算看是不是真的会有输出，这就可以用到我们之前所说的前向传播，左下角便是前向传播算法向量化的过程。

前向传播使得我们能够计算出神经网络中每一个神经元的激活值。

![](/pic/MachineLearning/神经网络反向传播之反向传播3.png)

反向传播从直观上来说，就是对每一个节点，我们计算这样一项 $\delta_j^{(l)}$ ，代表了第 $l$ 层的第 $j$ 个节点的误差。

那么对每一个输出单元（以右上角的网路模型为例）， $\delta_j^{(4)} = a_j^{(4)}-y_j$ ，其中 $a_j^{(4)} = (h_\theta(x))_j$，向量化表示即为 $\delta^{(4)} = a^{(4)}-y$ 。

然后给出了下列式子(并未加以证明)：
$$
\delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)}.*g'(z^{(3)})
$$
其中 $g'(z^{(3)}) = a^{(3)}.*(1-a^{(3)})$ ，这个利用高数知识求导可以很简单求出。

然后类似的可以得知：
$$
\delta^{(2)} = (\Theta^{(2)})^T\delta^{(3)}.*g'(z^{(2)})
$$
然后并没有 $\delta^{(1)}$ ，因为 $\delta^{(1)}$ 就是给定的输入，并没有误差。

接着又不加证明地给出了
$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta) = a_j^{(l)}\delta_i^{(l+1)}
$$
这样我们便可以从输出层反向求出每一项 $\theta$ 的偏导数（目前所讲的并未提到正则化系数 $\lambda$ 和正则项，即 $\lambda = 0$ ），这便是反向传播算法。

![](/pic/MachineLearning/神经网络反向传播之反向传播4.png)

上面是反向传播的具体过程，要注意 $j = 0$ 时为偏置单元，不须考虑正则项。

