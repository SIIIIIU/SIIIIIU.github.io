---
layout: article
title: 神经网络学习
tags: Machine-Learning
article_header:
  type: overlay
typora-root-url: ..

---



<!--more-->

# 神经网络学习

[TOC]

## 七、神经网络学习

### 1、非线性假设

![](/pic/MachineLearning/神经网络学习之非线性假设.png)

在之前的学习中，对于分类问题和回归问题我们都有了各自的方法进行解决，那么为什么还要学习神经网络模型呢？事实上，在实际的学习中，我们可能会有很多个特征，比如还是卖房子的问题，我们有大小，卧室，楼层，房龄等100个特征，那么如果我们再用构造多项式来解决这个非线性问题的话，光二次项就有 $\frac{n^2}{2}$ 个也就是5000个，加上三次项也就更多，这样运算的速度就会非常慢。

![](/pic/MachineLearning/神经网络学习之非线性假设2.png)

比如在计算机视觉中如果我们要做一个汽车识别器，我们具体应该怎样做呢？

由于对于计算机来说，一个个图片就是一个个像素点，无法像人类一样直接感知，所以一般是这样的做法，取其中的两个像素点，如果是灰度图像的话那么会有0-255总共有256个值，而如果是rgb图像的话就会有3x256个值，然后对这两个像素点进行分类分析。然后同理对图片中的所有像素点使用同样的方法。假设是一个50x50的图片，就有近3千万个特征，这样的数量实在是太大，像之前的Logistic回归模型就难以解决。



### 2、模型展示

这节课我们来讲如何运用神经网络，或者说在使用神经网络模型时该如何表示我们的假设或者模型。

![](/pic/MachineLearning/神经网络学习之模型展示.png)

首先介绍下生物中的神经元，如上图所示，神经元有一个树突接收外界传过来的脉冲刺激，可以理解为输入通道，有一个胞体作为计算单元，然后将处理过的脉冲信息通过轴突（输出通道）传递给其他神经元。

![](/pic/MachineLearning/神经网络学习之模型展示2.png)

而多个神经元正像我们刚刚所说的一样，轴突将脉冲信息传递给其他神经元的树突。

![](/pic/MachineLearning/神经网络学习之模型展示3.png)

计算机中我们用上面这个逻辑单元模拟神经元，其中左边表示传入通道， $x_1,x_2,x_3$ 表示传入的参数，然后神经元做一些计算，并通过传出通道输出计算结果。当画出上图这样的图画时，就表示
$$
h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}
$$
通常， $x$ 和 $\theta$ 是我们的参数向量。

一般来说我们只绘制 $x_1,x_2,x_3$ 这些参数，只有当有必要的时候才会绘制 $x_0$ 这个参数（$x_0$ 恒等于1）。 $x_0$ 有时也被成为偏置单元或者偏置神经元。

有时候我们会说这是一个带有 sigmod 或者 Logistic 激活函数的人工神经元，在神经网络术语中，激活函数是指代非线性函数 g(z) 的另一个术语。
$$
g(z) = \frac{1}{1+e^{-z}}
$$
注意模型的参数 $\theta$ 有时也被成为权重(weight)。

![](/pic/MachineLearning/神经网络学习之模型展示4.png)

神经网络就是一组神经元连接在一起的集合。

具体来说，这是我们的输入单元 $x_1,x_2,x_3$ ，有时也可以画上额外的节点 $x_0$ 。

后面有三个神经元 $a_1^{(2)},a_2^{(2)},a_3^{(2)}$ ，同样也可以加上一个偏置单元 $a_1^{(2)}$ ，它的值也恒为1。

然后在最后一层我们还有一个神经元，正是这个神经元输出了假设函数 $h_\theta(x)$ 的计算结果。

其中第一层成为输入层，最后一层称为输出层，中间的层都称为隐藏层。

![](/pic/MachineLearning/神经网络学习之模型展示5.png)

接下来让我们分析这个表所表示的具体的计算步骤。

先定义一些术语：

 $a_i^{(j)}$ 表示第 $j$ 层的第 $i$ 个神经元的激活项，所谓激活项，是指由一个具体神经元计算并输出的值。

另外，我们的神经网络被 $\Theta $ 矩阵参数化。 $\Theta_j$ 就是一个权重矩阵，它控制从某一层，比如第一层到第二层，第二层到第三次，的映射。

具体来说，比如 $a_1^{(2)}$ 就是 sigmod 函数作用在 $\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3$ 这种线性组合上的结果。

**其中 $\Theta^{(1)}$ 就表示第一层映射到第二层的参数矩阵。由于第二层有 $a_1^{(2)},a_2^{(2)},a_3^{(2)}$ 三个神经元需要由第一层的输入通过线性组合而得出（偏置单元 $a_0^{(2)}$ 固定为1），故 $\Theta^{(1)}$ 有三行。另外第二层的每个神经元是由第一层内包括偏置单元在内的四个神经元线性组合得出的，所以 $\Theta^{(1)}$ 有四列。**

**总结一下：**

**如果神经网络的第 $j$ 层有 $s_j$ 个单元，在第 $j+1$ 层有 $s_{j+1}$ 个单元，那么参数矩阵 $\Theta^{(j)}$ 的维度就是 $s_{j+1}\times (s_j+1)$ 。**

下面我们展示一个向量化的计算方法：

![](/pic/MachineLearning/神经网络学习之模型展示6.png)

首先我们定义一些额外的项，我们定义 $\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3 = z_1^{(2)}$ ，将 $a_1^{(2)},a_2^{(2)},a_3^{(2)}$ 列为一个三维的列向量 $a^{(2)}$ ，则 

$$
z^{(2)} = \Theta^{(1)}a^{(1)}
$$

$$
a^{(2)} = g(z^{(2)})
$$

那么同理对于第三层的神经元我们就可以进行下列推倒：
$$
Add \quad a_0^{(2)} = 1
$$

$$
z^{(3)} = \Theta^{(2)}a^{(2)}
$$

$$
h_\Theta(x) = a^{(3)} = g(z^{(3)})
$$



这个计算 $h_\Theta(x)$ 的值的过程，也被成为前向传播，这样命名是因为我们从输入单元的激活项开始，然后前向传播给隐藏层，计算隐藏层的激活项，然后继续前向传播，计算输出层的激活项。

这个依次计算激活项，从输入层到隐藏层再到输出层的过程，叫做前向传播。我们刚刚推导的是这一过程的向量化实现方法。

![](/pic/MachineLearning/神经网络学习之模型展示7.png)

我们如果把左边输入层遮住，只看右边，那么右边看起来就像一个逻辑回归（其实就是逻辑回归）， $a_1^{(2)},a_2^{(2)},a_3^{(2)}$ 是这个逻辑回归的输入，则 
$$
h_\Theta(x) = g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(2)}a_3^{(2)})
$$
但它不是用原本的 $x_1,x_2,x_3$ 作为特征，而是用 $a_1^{(2)},a_2^{(2)},a_3^{(2)}$ 作为新的特征。

而 $a_1^{(2)},a_2^{(2)},a_3^{(2)}$ 则是通过学习第一层来获得的。所以在神经网络中，它不是使用输入 $x_1,x_2,x_3$ 训练逻辑回归，而是自己训练逻辑回归的输入 $a_1^{(2)},a_2^{(2)},a_3^{(2)}$ 。就可以想象，根据为 $\Theta^{(1)}$ 选择不同的参数，有时可以学习到一些很有趣或者很复杂的特征，就可以得到一个很好的假设函数。

你也可以使用多项式，比如 $x_1x_2$ 作为输入，但这个算法可以灵活地尝试快速学习任意的特征项。

![](/pic/MachineLearning/神经网络学习之模型展示8.png)

神经网络中神经元的连接方式，被成为神经网路的架构。



### 3、多元分类

要在神经网络中实现多元分类，采用的方法本质上是一对多法的拓展。

![](/pic/MachineLearning/神经网络学习之多元分类.png)

假设有一个计算机视觉的例子，我们不止要识别汽车，还要识别行人，摩托车和卡车。

那么我们要做的就是，建立一个有四个输出单元的神经网络，现在神经网络的输出，将是一个含有四个数的向量，输出变为了一个思维的向量。现在我们要做的就是，用第一个输出单元判断是否为行人，第二个判断是否为汽车，依次类推。当图片中是个行人的时候，理想的情况下会输出向量
$$
\begin{bmatrix}
1 \\
0\\
0\\
0
\end{bmatrix}
$$
这其实就像我们在逻辑回归时提到的一对多法。现在可以说我们有四个逻辑回归分类器，其每一个来判断是不是其对应的那一类。

![](/pic/MachineLearning/神经网络学习之多元分类2.png)

