---
layout: article
title: Logistic回归
tags: Machine-Learning
article_header:
  type: overlay
typora-root-url: ..

---



<!--more-->

# Logistic回归

[TOC]

## 五、Logistic回归

### 1、分类

这节课开始要讨论的是要预测的变量y是一个离散值情况下的分类问题。

![](/pic/MachineLearning/logistics回归之分类.png)

假设有这样的场景：

1、邮件系统中判断邮件是否是垃圾邮件

2、线上交易系统中判断对方账户是否为欺诈账户

3、判断肿瘤是良性肿瘤还是恶性肿瘤

在上述的场景下输出只会有 0、1这两种离散情况，这就被称为分类问题。

通常情况下将0称为负类，1称为正类，0通常表示没有某样东西的情况，1则表示有某样东西的情况。

现在我们只讨论只有0和1这两种情况的例子，即二分类（二元分类），以后会考虑多分类的情况，即 $y$ 取值0、1、2、3等其他值的情况。 	

![](/pic/MachineLearning/logistics回归之分类2.png)

给定上图的数据集，即关于肿瘤大小与肿瘤恶性良性的数据集。

首先我们可以用我们学过的线性回归算法进行试验。

开始只有前8个数据集，根据线性回归拟合的假设函数曲线 $h_\theta(x) = \theta^Tx$ 如图粉色线所示，那么我们如何做出预测呢，我们可以将分类器的阈值设为 0.5 ，作垂线如图粉色垂线所示，那么也就是，当 $h_{\theta}(x)$ 的值大于等于0.5时，做出预测 $y=1$ ，即为恶性，反之若小于 0.5 ，则做出预测 $y=0$ ，即为良性。现在看起来这个假设函数对于现有数据集拟合的很好，但当我们多加入一个数据（即最后一个数据），则线性回归的假设函数如图蓝色所示，那么当阈值仍为 0.5 时，作垂线为蓝色垂线出，此时该垂点左边并不都是良性肿瘤，这就是一个很差劲的拟合函数了。

所以对于一个分类问题的数据集进行线性回归有时候效果会好，但通常不是一个好方法。所以通常不推荐将线性回归用在分类问题上。

![](/pic/MachineLearning/logistics回归之分类3.png)

另外一个问题，如果对分类问题进行线性回归，通常输出的值是要远大于1或者远小于0的，即使所有数据集的值都为0或者1，这就很怪。所以接下来会开发一个名为logistics回归的算法，其特点为算法的输出值均为0和1之间的值，即不会小于0也不会大于1。btw，logistics回归算法是一种分类算法，并不是一种回归算法，名称是因为一些历史遗留问题。

### 2、假设陈述

![](/pic/MachineLearning/logistics回归之假设陈述.png)

对于线性回归来说，假设函数为 $h_{\theta}(x) = \theta^Tx$ ,而logistics回归是将 $\theta^Tx$ 作为参数，用 $g(z)$ 函数进行计算， $g(z)$ 的公式为 $g(z) = \frac{1}{1+e^{-z}}$ ， $g(z)$ 被称为 Sigmoid函数或者Logistic函数。

 $g(z)$ 函数的函数图像如右下角所示。

用更严谨的数学公式表示下：

![](/pic/MachineLearning/logistics回归之假设陈述2.png)

像一般情况一样， $x$ 为一个向量， $x_0 = 1$ ， $x_1$ 为肿瘤的大小，而 $h_{\theta}(x)$ 的值则表示肿瘤是恶性肿瘤的概率。

用概率论的方法来表示则为

​                                                       $h_{\theta}(x) = P(y=1|x;\theta)$  

含义为： $y=1 $ ，给定 $x$ 和概率的参数  $\theta$ 时的概率，其中 $\theta$ 的作用是，通过给定的 $\theta$ ，使得样本满足正类时 $\theta x$ 尽可能的大，样本属于负类时 $\theta x$ 尽可能的小。  

### 3、决策界限

![](/pic/MachineLearning/logistics回归之决策边界.png)

像之前所说的logistic回归 $h_{\theta}(x) = g(\theta^Tx)$ 的曲线如上图所示，而 $h_{\theta}(x)$ 表示的是为恶性肿瘤的概率，但我们要解决的问题是一个分类问题，输出值 $y$ 只能有 0 和 1 这两种值，那么何时取 1 ，何时取 0 呢？

一般来说，当概率大于 0.5 时我们认为其更有可能发生，所以我们就预测，若 $h_{\theta}(x)>0.5$ ，则输出为1，反之则输出为0。

观察 Sigmoid函数 $g(z)$ 曲线可以很容易的看出， $z=0$ 处 $g(z)$ 的取值为0.5，即 z 小于0时， $g(z)<0.5$ ，y = 0；z 大于等于0时， $g(z)>=0.5$ ，y = 1。

又因为 $h_{\theta}(x) = g(\theta^Tx)$ ，所以 $\theta^Tx<0$ 时， $h_{\theta}(x) = g(\theta^Tx)$ < 0.5，y = 0； $\theta^Tx>=0$ 时， $h_{\theta}(x) = g(\theta^Tx)$ >= 0.5，y = 1.

![](/pic/MachineLearning/logistics回归之决策边界2.png)

假设我们有左上这样一个数据集，同时假设假设函数为 $h_{\theta}(x) = g(\theta_0+\theta_1x_1+\theta_2x_2)$ ,目前还没有提到如何拟合模型中的参数，将在后面的章节提及。但是假设我们已经拟合好了参数，最终选择如下值： $\theta_0=-3,\theta_1 = 1,\theta_2 = 1$ 。

根据上面所论述的，若 $y = 1$ ，则 $h_{\theta}(x) = g(\theta^Tx)>0.5$ ，即 $\theta^Tx>= 0$ ，即 $-3+x_1+x_2>=0$ ，即 $x_1+x_2>=3$ 。而 $x1 + x_2 = 3$ 在图中则为那条洋红色的直线。这条直线则表示 $h_{\theta}(x) = 0.5$ ，这条直线也就是决策边界。

需要澄清一点，决策边界是假设函数的一个属性，它包括参数 $\theta_0,\theta_1,\theta_2$ 。决策边界决定于其参数，不是数据集的属性。

![](/pic/MachineLearning/logistics回归之决策边界3.png)

让我们看一个更复杂的情况，给出如左上给出的数据集，很明显，我们很难用一条直线将上图的决策界限表示出来。之前在多项式回归和线性回归中，我们提到可以在特征中添加额外的高阶多项式项，在logistic回归中同样可以这样做。

具体来说，假如假设函数如上图所示
$$
h_\theta(x) = g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2)
$$
即我们已经添加了两个额外的特征 $x_1^2$ 和 $x_2^2$ 。我们将在后面的章节讲解如何选取合适的参数 $\theta_0,\theta_1,\theta_2,\theta_3,\theta_4$ ，假设我们现在已经做过该操作，并且得出 $\theta_0 = -1,\theta_1 = 0,\theta_2 = 0,\theta_3 = 1,\theta_4 = 1$ ，根据之前的结论，即 $-1+x_1^2+x_2^2 \ge 0$ 时，$h_{\theta}(x) \ge 0.5$ ，$y = 1$ 。

 $x_1^2+x_2^2 = 1$ 即为决策界限。

同理，如果有更多的高阶多项式，就可以得到更复杂的决策边界，而logistic回归可以用来寻找决策边界。

### 4、代价函数

 对这个代价函数的理解是这样的，它是在输出的预测值是h(x)时，而实际标签是y的情况下，我们希望学习算法所付出的代价。

![](/pic/MachineLearning/logistics回归之代价函数.png)

这一章我们讨论如何选取合适的参数 $\theta$ ，即参数 $\theta$ 的拟合问题。

也就是这样的场景：

我们有 $m$ 个训练集，所以我们有了一个 $n+1$ 维的特征值向量 $x$ ，其中 $x_0 = 1$ ，而输出值 $y$ 只有 0 和 1 两种取值。另外假设函数为
$$
h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}
$$


![](/pic/MachineLearning/logistic回归之代价函数2.png)

在之前的线性回归中我们使用的是这样的一个代价函数：
$$
J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
将其中的 $\frac{1}{2}$ 放到求和之中，变形为：
$$
J(\theta) = \frac{1}{m}\sum\limits^m_{i=1}\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
并将求和中的部分提取为另一个函数：
$$
Cost(h_{\theta}(x),y) = \frac{1}{2}(h_\theta(x)-y)^2
$$
其中因为 $h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}$ ，是一个非线性函数，其平方之后会变成如左下角图中所示的函数。这种函数被称为非凸函数（与右边凸函数相比较），它存在非常多的局部最优解，这就导致若使用梯度下降算法拟合参数无法保证求出的是我们所期望的最优参数（甚至可能相差极大），于是对于logistic回归算法我们提出另外一种代价函数。

![](/pic/MachineLearning/logistic回归之代价函数3.png)

这种代价函数为：
$$
Cost(h_{\theta}(x),y) = \begin{cases} 
-log(h_{\theta}(x)) \quad if \quad y = 1\\ 
-log(1-h_{\theta}(x)) \quad if \quad y=0 
\end{cases}
$$
==注：我觉得公式中的 $log$ 应为 $ln$ ，查阅相关资料后也为 $ln$ 。==

这个代价函数乍一看很复杂，但将函数图像画出来之后便非常清晰。

![](/pic/MachineLearning/logistic回归之代价函数4.png)

当 $y = 1$ 时函数图像如上图所示。

当 $h_\theta(x)$ 越接近 1 时， $Cost$ 函数的值越接近0，即表示为 1 的可能性越大，误差越小，反之 $h_\theta(x)$ 越接近0，则值接近无限大，即表示为 1 的可能性越小，误差越大。

$y = 0$ 时同理。

不过当 $y = 1$ 时，即表示 $h_{\theta}(x) \ge 0.5$ ，所以我认为函数图像中小于0.5的部分其实并不会出现，老师课程中并没有提起这个。 

### 5、简化代价函数与梯度下降

![](/pic/MachineLearning/logistic回归之简化代价函数.png)

在上一节我们提到过了logistic回归的代价函数为：
$$
Cost(h_{\theta}(x),y) = 
\begin{cases} 
-log(h_{\theta}(x)) \quad if \quad y = 1\\ 
-log(1-h_{\theta}(x)) \quad if \quad y=0 
\end{cases}
$$

由于logistic回归针对的是分类问题，所以 $y$ 的取值只有 0 和 1 ，所以我们可以根据这个特性，将上述的代价函数合并为一个式子：
$$
Cost(h_{\theta},y) = -ylog(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))
$$
这个式子是可以很显然的推出来的，根据 $y$ 的取值。

![](/pic/MachineLearning/logistic回归之简化代价函数2.png)

于是我们便将logistic回归的代价函数简化为了上述式子，接下来我们要做的就是去寻找合适的参数 $\theta$ ，即拟合参数 $\theta$ ，使得代价函数的值 $J(\theta)$ 的值最小。接下来当给我们一个新的输入 $x$ ，我们就会根据我们的模型（假设函数），输入相应的预测，在这个问题中即该肿瘤是良性的概率。

![](/pic/MachineLearning/logistic回归之梯度下降.png)

那么如何拟合参数 $\theta$ 呢？那就是使用之前在线性回归模型中所学的梯度下降算法。

其中在梯度下降算法中，在找到最优解之前，会一直循环：
$$
\theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$
其中 
$$
\alpha\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$
具体推倒如下：[^1]
$$
J(\theta) = -\frac{1}{m}[\sum\limits_{i=1}^my^{(i)}ln(h_{\theta}(x^{(i)}))+(1-y^{(i)})ln(1-h_{\theta}(x^{(i)}))] \\
h_{\theta}(x^{(i)}) = \frac{1}{1+e^{-g(x^{(i)})}}  \\ 
g(x^{(i)}) = \theta_0 + \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + \dots + \theta_nx_n^{(i)}  \\ 
[y^{(i)}\ln(h_{\theta}(x^{(i)})) + (1-y^{(i)})\ln(1-h_{\theta}(x^{(i)}))]'\\
= y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}[h_{\theta}(x^{(i)})]' - (1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}[h_{\theta}(x^{(i)})]' \\ 
= [y^{(i)}\frac{1}{h_{\theta}(x^{(i)})} - \frac{1}{1-h_{\theta}(x^{(i)})} + y^{(i)}\frac{1}{1-h_{\theta}(x^{(i)})}][h_{\theta}(x^{(i)})]' \\
\because h_{\theta}(x^{(i)}) = \frac{1}{1+e^{-g(x^{(i)})}}  \\  
\therefore = [y^{(i)}\frac{(1 + e^{-g(x^{(i)})})^2}{e^{-g(x^{(i)})}} - \frac{1 + e^{-g(x^{(i)})}}{e^{-g(x^{(i)})}}][\frac{e^{-g(x^{(i)})}}{(1 + e^{-g(x^{(i)})})^2}g'(x^{(i)})] \\ 
= (y^{(i)} - \frac{1}{1 + e^{-g(x^{(i)})}})g'(x^{(i)}) \\ 
= (y^{(i)} - h_{\theta}(x^{(i)}))g'(x^{(i)}) \\
\because g(x^{(i)}) = \theta_0 + \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + \dots + \theta_nx_n^{(i)} \\
\therefore 当对 \theta_j求偏导时,g'(x^{(i)}) = x_j^{(i)} \\ 
\therefore = (y^{(i)} - h_{\theta}(x^{(i)}))x_j^{(i)} \\ 
\therefore \alpha\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} （原式得证）
$$


![](/pic/MachineLearning/logistic回归之梯度下降2.png)



### 6、高级优化

![](/pic/MachineLearning/logistic回归之高级优化.png)

在使用梯度下降拟合参数的过程中，我们知道 代价函数 $J(\theta)$ ，希望求得参数向量 $\theta$ ，使得 $J(\theta)$ 的值最小。

所以对于参数向量 $\theta$ ,我们需要写出相应的代码来计算代价函数 $J(\theta)$ 以及其导数的值。

然后使用梯度下降算法，循环更新参数向量 $\theta$ ，直至代价函数对各参数变量的导数均为0。

但其实梯度下降并不是我们唯一的选择，我们还有一些高级的优化算法来供我们使用。

![](/pic/MachineLearning/logistic回归之高级优化2.png)

比如上图中提到的共轭梯度算法（Gradient descent），BFGS算法和L-BFGS算法。

上述提到的三种高级优化算法，他们不但相比梯度下降算法收敛更快，而且并不需要手动的去选择学习率 $\alpha$ ，会自动的选择合适的学习率 $\alpha$ ，甚至会在每次迭代中重新选择合适的学习率 $\alpha$ 。而相比较梯度下降算法其缺点就是更加复杂，而且不同的库对于上述算法的实现效果甚至都不尽相同，所以在实际使用中可以多尝试几个库来选取最优的一个。

![](/pic/MachineLearning/logistic回归之高级优化3.png)

那么具体如何使用这种高级优化算法呢？

以一个简单的代价函数为例。

上图是老师在octave中所使用的代码，注意在MATLAB中optimset函数里的迭代次数100不可加单引号。

在具体使用中我们首先需要定义一个方法，方法的返回值有两个，分别是 $jVal$ （给定参数 $\theta$ 代价函数的值）, $gradient$ （一个数组，保存了每个参数 $\theta$ 对代价函数 $J(\theta)$ 求偏导后的式子），在函数中需要将求两个返回值的代码写出来。

然后定义一个options结构体，可以保存多个参数名称和值，并且这个结构体可以作为参数传递给函数。

上图在结构体中定义了两个参数及其值，其中 $GradObj$ 的含义是是否使用自定义的梯度函数，而 $on$ 的含义即为使用。后面 $MaxIter$ 即为最大循环次数。

接着初始化一个列向量表示参数 $\theta$ （函数使用前必须初始化参数向量）。

然后带入高级优化函数 $fminunc$ ，其返回值有三个，第一个为运行出的结果参数向量 $\theta$ ，第二个为代价函数的值，第三个为退出的标志符，我的理解是类似c语言的返回值，1的话为正常结束。

![](/pic/MachineLearning/logistic回归之高级优化4.png)

那么如何在logistic回归模型中使用呢？

首先定义一个参数向量 $\theta$ ，其初始化了相应个数的参数变量。然后像之前所说的那样定义一个函数，该函数中需要将代价函数对各个参数求偏导后的式子用代码写出来，然后和之前的步骤一样。

所以使用这个高级优化函数最主要的就是自定义一个函数，然后在函数中将代价函数和对代价函数求偏导的代码写出来。

### 7、多元分类：一对多

![](/pic/MachineLearning/logistic回归之多元分类.png)

接下来我们要讨论多元分类的问题，即输入有两个以上的离散值。

例如像这样的场景：

对邮件进行分类，我们经常需要对收到的邮件进行各种各样的分类，比如说分成工作，朋友，家庭，爱好，我们需要将一封邮件分到上述四种类别之中，这就是一个具有四个分类目标的分类问题，即多元分类问题。

![](/pic/MachineLearning/logistic回归之多元分类2.png)

像之前的二元分类问题中，如左图所示有两种类别的数据集，我们可以通过假设函数确定决策界限，然后判断属于哪一种类别。

而多元分类问题则像右图所示，有两个以上类别的数据集，我们有多个分类目标，这种情况下我们如何对其分类呢？其实依旧是使用logistic回归算法，不过同时还要使用一对多的方法。

![](/pic/MachineLearning/logistic回归之多元分类3.png)

具体的过程是这样的：

假设我们有一个三种类别的数据集，其中三角形代表第一种类别，方形代表第二种类别，叉代表第三种类别。

然后对于第一种类别，我们构造一个分类器 $h^{(1)}_{\theta}(x)$ ，而第一种类别（三角形）为正样本，其他两种类别（圆形）为负样本，这样分类器 $h_{\theta}^{(1)}(x)$ 的值便是是样本一（三角形）的概率。

同理构造另外两个分类器 $h^{(2)}_{\theta}(x)、h^{(3)}_{\theta}(x)$ 分别表示是第二种类别（方形）和第三种类别（叉）的概率。

这样我们一共得到了三个分类器。

接着便是将预测样本分别带入三个分类器，其中值最大的一个即表示为该类别的概率最大，输出结果便是该类别。

![](/pic/MachineLearning/logistic回归之多元分类4.png)