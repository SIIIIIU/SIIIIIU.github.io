---
layout: article
title: 吴恩达机器学习之多变量线性回归
tags: Machine-Learning
article_header:
  type: overlay
typora-root-url: ..

---



<!--more-->

# 多变量线性回归

[TOC]

## 四、多变量线性回归

适用于多变量或者多特征量的情况。

### 1、多功能

![](/pic/MachineLearning/单变量线性回归模型假设函数.png)

若只有一个变量那我们的假设函数是这样的。

而若有多个特征值，那么假设函数就变为了：

​                                            $h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n$ 

为了方便表示，我们假设了一个并不存在的特征值 $x_0$ ,且 $x_0$ 恒为 $1$ ，那么 $x$ 就可以表示为一个 $n+1$ 维的向量：

​                                                                 $x = \begin{bmatrix}x_0\\x_1\\x_2\\ \vdots\\x_n\end{bmatrix} \in R^{n+1} $ 

同时参数 $\theta$ 也可以表示为一个 $n+1$ 维的向量：

​                                                                 $\theta = \begin{bmatrix}\theta_0\\\theta_1\\\theta_2\\ \vdots\\\theta_n\end{bmatrix} \in R^{n+1} $ 

那么假设函数就变成了：

​                                         $h_\theta(x) = \theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n$ 

​                                                      $=\theta^Tx$ 

### 2、多元梯度下降法

#### 2.1、定义

多变量线性回归模型的代价函数定义如下：

![](/pic/MachineLearning/多变量线性回归模型代价函数定义.png)

和单变量梯度下降法类似，多变量梯度下降法的具体过程如下：

![](/pic/MachineLearning/多变量梯度下降法过程.png)

（记得为同步更新）

#### 2.2特征缩放

![](/pic/MachineLearning/多变量梯度下降之特征缩放.png)

特征缩放就是让多个特征值在近似的范围之内，进行梯度下降。

以上图左边两个特征值为例， $x_1$ 的范围是 $0-2000$ ， $x_2$ 的范围是 $1-5$ ，那么不考虑 $\theta_0$ ，代价函数的等高线图如上图左边所示，因为 $x_1,x_2$ 的范围差距极大，所以是一个狭长的椭圆形，那么梯度下降的过程如红线所示，就会左右来回震荡，下降速度缓慢。所以为提高下降速度，将 $x_1$ 的数值除以2000， $x_2$ 的数值除以5，那么等高线图如右图所示，是一个正圆，梯度下降的速度也就较左边快得多。

![](/pic/MachineLearning/多变量梯度下降之特征缩放特征值范围.png)

把特征值缩放到 -1到1的范围就可以了，不过要求并不十分严格，只要近似就好了，以老师的经验，最大-3到3都是OK的，最小 $-\frac{1}{3} $ 到 $\frac{1}{3}$ 就可以。

除了将特征值除以最大值之外，在特征缩放中有时也会进行一项称为均值归一化的工作。

![](/pic/MachineLearning/多变量梯度下降之均值归一化.png)

均值归一化就是将特征值 $x_i$ 替换为 $x_i-\mu_i$ ，使你的特征值具有为0的平均值。

一个一般化的公式就是这样：

​                                                                     $\frac{x_i-\mu_i}{s_i} \rightarrow x_i$    

其中 $\mu_i$ 就是 $x_i$ 取值范围的中位数， $s_i$ 就是 $x_i$ 的区间长度（最大值减最小值）。

==注意：由于特征值缩放的目的是减少梯度下降的路径以加快梯度下降的速度，所以特征值缩放的精度并不精确，只要是大体的范围之内便可。==

#### 2.3学习率

![](/pic/MachineLearning/多变量梯度下降之判断收敛.png)

对于一个多变量的梯度下降算法，我们如何判断结果是否已经收敛呢？

如上图左边所示，代价函数根据迭代次数的变化映射在图中，由图可知，在300次迭代知乎，代价函数下降的值小的可以忽略不计，这时便称已经收敛。

由于不同场景所需的迭代次数差别非常大，所以我们不能仅仅根据迭代次数进行判断，一般是画出右图这样的函数图，根据函数曲线进行判断。

或者如右图所示，有这样的一个自动测试程序，当迭代后下降的小于一个给定的很小很小的值 $\epsilon$ ，便视为已经收敛。

不过由于这样的一个 $\epsilon$ 的值很难确定，所以推荐使用左边的方法，即画出函数曲线。



![](/pic/MachineLearning/多变量梯度下降之α大小.png)

通过画出代价函数的下降曲线可以很方便的看出代价函数是否在正确运行。

如上图中左上的函数，代价函数的值在不断下降，这时我们一般应该选取一个更小的学习率 $\alpha$ 值，因为导致该图的原因一般为选取的 $\alpha$ 值过大，超过了最小值，并不断左右震荡上升（如右图橙线所示）。

对于左下的函数，也应选取一个更小的函数，数学家已经证明过了，课上没有证明。

==对于一个足够小的 $\alpha$ 值，在所有的迭代中代价函数的值都会下降。不过如果 $\alpha$ 的值过小，代价函数收敛就会很慢。==

![](/pic/MachineLearning/多变量梯度下降之学习率选取.png)

总结一下：

==如果 $\alpha$ 的值过小，那么代价函数的收敛就会很慢。==

==而如果 $\alpha$ 的值过大，那么代价函数的值在每次迭代中不一定会下降，代价函数也可能不会收敛。==

老师选取 $\alpha$ 的方法，一般是找到一个对于该代价函数来说过大的 $\alpha$ 中最小的一个，以及对于该代价函数来说过小的 $\alpha$ 中最大的一个，然后在该区间内以3的倍数寻找 $\alpha$ ，这样一般能找到一个比较合理的 $\alpha$ 。

### 3、特征和多项式回归

本节要讨论多个不同的特征如何选取合适的特征，以及如何得到不同的算法。当选择了合适的特征后，这些算法往往是非常有效的。

多项式回归能够用线性回归的方法进行拟合非常复杂的函数，甚至是非线性函数。

以预测房价为例：

![](/pic/MachineLearning/多变量线性回归模型之选择特征.png)

上图的例子给了我们临街宽度 $x_1$ 和纵向深度 $x_2$ 两个特征，但我们很直观的可以知道，和房价更相关的是房屋面积，所以我们可以只用两者的乘积，即房屋面积这一个特征。有时可以通过定义一个新特征，可以得到一个更好的模型。

与选择特征密切相关的一个概念，被称为多项式回归。

![](/pic/MachineLearning/多变量之多项式模型.png)

比如有如图所示的房价数据集，通过观察数据集你觉得直线并不能很好的拟合数据，因此你很容易就会想到二次模型，但你很快会觉得二次模型不合理，因为二次函数会降下来，但随着房屋面积的增大，房屋的价格并不会降下来。所以可能我们会选择不同的多项式模型，并转而选择一个三次函数。我们用这个三次函数的式子可以得到这样的模型（绿色），这个模型和数据集拟合的更好，因为最终不会下降。

那么我们如何进行拟合呢？

我们将 $size$ 作为 $x_1$ ， $size^2$ 作为 $x_2$ ， $size^3$ 作为 $x_3$ ，这样来进行拟合。

因为这个模型是一个三次模型，所以特征缩放就非常重要。

![](/pic/MachineLearning/特征选择.png)

事实上我们有很大的余地来选择使用哪些特征，如上图我们可以平方根函数来进行拟合。

### 4、正规方程

正规方程对于某些线性回归问题，可以提供更好的方法来求得参数 $\theta$ 的最优值。

![](/pic/MachineLearning/正规方程与梯度下降比较.png)

与梯度下降这种一步一步迭代求出 $\theta$ 最优值的解法不同，正规方程提供了一种可以一步直接求出 $\theta$ 最优值的解析法。

![](/pic/MachineLearning/正规方程过程.png)

以最简单的一种情况为例：

假设 $J(\theta)$ 是一个 $\theta$ 为标量的二次函数，那么如果求最低点的 $\theta$ 值呢？

学过微积分就很容易可以想到，对 $J(\theta)$ 进行求导，求出令 $J'(\theta)=0$ 的那一点的 $\theta$ 值就可以了。

同理，推广到多个特征 $\theta$ 的情况，只要对 $J(\theta)$ 的各个 $\theta$ 求偏导，并使各个偏导等于0，求出该条件下各 $\theta$ 的取值就可以了。

![](/pic/MachineLearning/正规方程过程2.png)

 其中，将各样本和价格用矩阵表示如上图所示。

根据：

​                                                       $J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$ 

其中： 

​	                                          		$h_{\theta}(x) = \theta^TX =\theta_0x_0+\theta_1x_1+...+\theta_nx_n$  

带入得：

​		                              				$J(\theta)=\frac{1}{2m}(X*\theta-Y)^T(X*\theta-Y)$

​                                                                 $=\frac{1}{2m}(Y^TY-Y^TX\theta-\theta^TX^TY+\theta^TX^TX\theta)$  

对上式进行求导并令 $J'(\theta) =0 $ 

求得                                             ==$\theta=(X^TX)^{-1}X^TY$==

![ ](picture\正规方程过程3.png)

![](/pic/MachineLearning/正规方程与梯度下降使用范围.png)

比较下梯度下降和正规方程这两种算法：

对于梯度下降算法而言：

1、由于梯度下降需要选择一个合适的学习率 $\alpha$ ，且不一定一次就可以找到合适的 $\alpha$ ，所以需要多次实验测试以找出一个合适的 $\alpha$ 。（缺）

2、梯度下降是一个迭代的算法，所以需要多次迭代。（缺）

3、即使特征值 $n$ 的数量很大也可以运行的很好。（优）

对于正规方程而言：

1、不需要去选择学习率 $\alpha$ 。（优）

2、不需要迭代。（优）

3、需要计算 $(X^TX)^{-1}$ ，时间复杂度大约在 $n$ 的三次方。（缺）

4、当特征值的数量n非常大时，会运行的非常慢。（缺）



以老师的经验，当 $n$ 为100或者1000的规模的时候，当然是选择正规方程算法，不过当到10000的规模的时候，会开始犹豫，可能会倾向于梯度下降算法，但也不绝对。但如果远大于10000的规模，那么基本就会选择梯度下降算法了。



### 5、正规方程在矩阵不可逆的情况下的解决办法

对于 $\theta=(X^TX)^{-1}X^TY$ ，很容易会考虑到一个问题，就是如果 $(X^TX)^{-1}$ 不可逆怎么办。事实上这个发生的情况很少，不过还是需要讨论一下的。

![](/pic/MachineLearning/正规方程不可逆情况.png)

 $(X^TX)^{-1}$ 不可逆的情况一般要考虑下面两个问题：

1、是否有多余的特征。

图中所给的例子就是既有特征平方，又有特征英尺，而1米等于3.28英尺，这就是多余的特征。

这个其实很好理解，就是矩阵对应成比例导致不满秩呗。

2、特征值过多。

如果特征值大于样本数量那么久很有可能出问题，这种情况可以删去一些特征或者使用正规化。